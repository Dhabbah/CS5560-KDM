{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text processing\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the csv file with amazon reviews\n",
    "reviews_df=pd.read_csv('dataset.csv',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['tweet'] = reviews_df['tweet'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              tweet\n",
      "0           0  RT @JeffreyGuterman: @DonaldJTrumpJr I think o...\n",
      "1           1  RT @RealJamesWoods: President Trump simply kno...\n",
      "2           2  RT @GOP: Tuesday’s #SOTU made it clear: Democr...\n",
      "3           3  @SenSanders Donald Trump left out a few detail...\n",
      "4           4  RT @SenTedCruz: .@realDonaldTrump's #SOTU spee...\n",
      "5           5  RT @RyanAFournier: How can a sitting United St...\n"
     ]
    }
   ],
   "source": [
    "print(reviews_df.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_clean(text):\n",
    "    \"\"\"\n",
    "    Function to clean text-remove punctuations, lowercase text etc.\n",
    "    \"\"\"\n",
    "    text = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", text)\n",
    "    text = text.lower()  # lower case text\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['news', 'say','use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do','took','time','year',\n",
    "'done', 'try', 'many', 'some','nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line','even', 'also', 'may', 'take', 'come', 'new','said', 'like','people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "     return [word for word in text if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text):\n",
    "    \"\"\"\n",
    "    Function to stem words\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "        text = [word for word in text if len(word) > 1] # no single letter words\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_all(text):\n",
    "    \"\"\"\n",
    "    This function applies all the functions above into one\n",
    "    \"\"\"\n",
    "    return stem_words(remove_stop_words(initial_clean(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean reviews and create new column \"tokenized\"\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean and tokenize 103055 reviews: 0.8351837515830993 min\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "reviews_df['tokenized_reviews'] = reviews_df['tweet'].apply(apply_all)\n",
    "t2 = time.time()\n",
    "print(\"Time to clean and tokenize\", len(reviews_df), \"reviews:\", (t2-t1)/60, \"min\") #Time to clean and tokenize 3209 reviews: 0.21254388093948365 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews with their respective tokenize version:\n",
      "   Unnamed: 0                                              tweet  \\\n",
      "0           0  RT @JeffreyGuterman: @DonaldJTrumpJr I think o...   \n",
      "1           1  RT @RealJamesWoods: President Trump simply kno...   \n",
      "2           2  RT @GOP: Tuesday’s #SOTU made it clear: Democr...   \n",
      "3           3  @SenSanders Donald Trump left out a few detail...   \n",
      "4           4  RT @SenTedCruz: .@realDonaldTrump's #SOTU spee...   \n",
      "\n",
      "                                   tokenized_reviews  \n",
      "0  [rt, one, absurd, moment, sotu, disgust, fathe...  \n",
      "1  [rt, presid, trump, simpli, knock, park, respo...  \n",
      "2  [rt, tuesday, sotu, made, clear, democrat, won...  \n",
      "3  [donald, trump, left, detail, sotu, 2018, lost...  \n",
      "4  [rt, sotu, speech, strong, start, incred, succ...  \n"
     ]
    }
   ],
   "source": [
    "print(\"reviews with their respective tokenize version:\" )\n",
    "print(reviews_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "import gensim\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Gensim dictionary from the tokenized data\n",
    "tokenized = reviews_df['tokenized_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating term dictionary of corpus, where each unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter terms which occurs in less than 1 review and more than 80% of the reviews.\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]]\n"
     ]
    }
   ],
   "source": [
    "#convert the dictionary to a bag of words corpus\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in tokenized]\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('absurd', 1), ('disgust', 1), ('father', 1), ('man', 1), ('moment', 1), ('one', 1), ('prob', 1), ('sotu', 1)]]\n"
     ]
    }
   ],
   "source": [
    "print([[(dictionary[id], freq) for id, freq in cp] for cp in corpus[:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean and tokenize 103055 tweet: 5.261151293913524 min\n"
     ]
    }
   ],
   "source": [
    "#LDA\n",
    "t1 = time.time()\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 4, id2word=dictionary, passes=15)\n",
    "t2 = time.time()\n",
    "print(\"Time to clean and tokenize\", len(reviews_df), \"tweet:\", (t2-t1)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "ldamodel.save('model_combined.gensim')\n",
    "topics = ldamodel.print_topics(num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now printing the topics and their composition\n",
      "This output shows the Topic-Words matrix for the 7 topics created and the 4 words within each topic\n",
      "(0, '0.043*\"sotu\" + 0.026*\"watch\" + 0.022*\"america\" + 0.021*\"call\" + 0.019*\"person\" + 0.018*\"israel\" + 0.017*\"hero\" + 0.017*\"antisemit\" + 0.017*\"democrat\" + 0.017*\"admit\"')\n",
      "(1, '0.062*\"sotu\" + 0.026*\"great\" + 0.021*\"choos\" + 0.020*\"trump\" + 0.019*\"ask\" + 0.019*\"must\" + 0.018*\"matter\" + 0.015*\"el\" + 0.012*\"en\" + 0.012*\"keep\"')\n",
      "(2, '0.066*\"sotu\" + 0.022*\"watch\" + 0.017*\"speech\" + 0.013*\"trump\" + 0.011*\"talk\" + 0.009*\"forward\" + 0.009*\"america\" + 0.009*\"presid\" + 0.008*\"follow\" + 0.008*\"politician\"')\n",
      "(3, '0.040*\"trump\" + 0.028*\"state\" + 0.024*\"sotu\" + 0.021*\"presid\" + 0.020*\"respons\" + 0.017*\"union\" + 0.017*\"women\" + 0.014*\"democrat\" + 0.012*\"well\" + 0.012*\"poll\"')\n"
     ]
    }
   ],
   "source": [
    "print(\"Now printing the topics and their composition\")\n",
    "print(\"This output shows the Topic-Words matrix for the 7 topics created and the 4 words within each topic\")\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "first review is:\n",
      "RT @JeffreyGuterman: @DonaldJTrumpJr I think one of the most absurd moments of the #SOTU is when your disgusting father, a man who has prob…\n",
      "\n",
      "\n",
      "The similarity of this review with the topics and respective similarity score are \n",
      "[(0, 0.33565742), (1, 0.028023863), (2, 0.6078731), (3, 0.028445657)]\n"
     ]
    }
   ],
   "source": [
    "#finding the similarity of the first review with topics\n",
    "print('\\n')\n",
    "print(\"first review is:\")\n",
    "print(reviews_df.tweet[0])\n",
    "get_document_topics = ldamodel.get_document_topics(corpus[0])\n",
    "print('\\n')\n",
    "print(\"The similarity of this review with the topics and respective similarity score are \")\n",
    "print(get_document_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\AppData\\Roaming\\Python\\Python36\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8889/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Mar/2020 19:17:01] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2020 19:17:01] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2020 19:17:01] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2020 19:17:01] \"GET /LDAvis.js HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "#visualizing topics\n",
    "lda_viz = gensim.models.ldamodel.LdaModel.load('model_combined.gensim')\n",
    "lda_display = pyLDAvis.gensim.prepare(lda_viz, corpus, dictionary, sort_topics=True)\n",
    "pyLDAvis.show(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
