{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the output (changes or transformations in the data) manually when the following tasks are applied on the input text. Show your output in details.\n",
    "\n",
    "## Installing the Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using five documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentA = 'the man went out for a walk'\n",
    "documentB = 'the children sat around the fire'\n",
    "documentC = 'the family went for a picnic in the park'\n",
    "documentD = 'the children were running in the park'\n",
    "documentE = 'the elder people sat around the fire'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from the texts in a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')\n",
    "bagOfWordsC = documentC.split(' ')\n",
    "bagOfWordsD = documentD.split(' ')\n",
    "bagOfWordsE = documentE.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing any duplicate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB)).union(set(bagOfWordsC)).union(set(bagOfWordsD)).union(set(bagOfWordsE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Creating a dictionary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1\n",
    "numOfWordsC = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsC:\n",
    "    numOfWordsC[word] += 1\n",
    "numOfWordsD = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsD:\n",
    "    numOfWordsD[word] += 1\n",
    "numOfWordsE = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsE:\n",
    "    numOfWordsE[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing useless words using the stopwords module\n",
    "## Term Frequency (TF)\n",
    "\n",
    "- The number of times a word appears in a document divded by the total number of words in the document. Every document has its own term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
    "tfC = computeTF(numOfWordsC, bagOfWordsC)\n",
    "tfD = computeTF(numOfWordsD, bagOfWordsD)\n",
    "tfE = computeTF(numOfWordsE, bagOfWordsE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'running': 0.0, 'man': 0.14285714285714285, 'park': 0.0, 'children': 0.0, 'fire': 0.0, 'in': 0.0, 'the': 0.14285714285714285, 'elder': 0.0, 'family': 0.0, 'went': 0.14285714285714285, 'were': 0.0, 'sat': 0.0, 'a': 0.14285714285714285, 'walk': 0.14285714285714285, 'for': 0.14285714285714285, 'people': 0.0, 'out': 0.14285714285714285, 'picnic': 0.0, 'around': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(tfA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'running': 0.0, 'man': 0.0, 'park': 0.0, 'children': 0.16666666666666666, 'fire': 0.16666666666666666, 'in': 0.0, 'the': 0.3333333333333333, 'elder': 0.0, 'family': 0.0, 'went': 0.0, 'were': 0.0, 'sat': 0.16666666666666666, 'a': 0.0, 'walk': 0.0, 'for': 0.0, 'people': 0.0, 'out': 0.0, 'picnic': 0.0, 'around': 0.16666666666666666}\n"
     ]
    }
   ],
   "source": [
    "print(tfB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'running': 0.0, 'man': 0.0, 'park': 0.1111111111111111, 'children': 0.0, 'fire': 0.0, 'in': 0.1111111111111111, 'the': 0.2222222222222222, 'elder': 0.0, 'family': 0.1111111111111111, 'went': 0.1111111111111111, 'were': 0.0, 'sat': 0.0, 'a': 0.1111111111111111, 'walk': 0.0, 'for': 0.1111111111111111, 'people': 0.0, 'out': 0.0, 'picnic': 0.1111111111111111, 'around': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(tfC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'running': 0.14285714285714285, 'man': 0.0, 'park': 0.14285714285714285, 'children': 0.14285714285714285, 'fire': 0.0, 'in': 0.14285714285714285, 'the': 0.2857142857142857, 'elder': 0.0, 'family': 0.0, 'went': 0.0, 'were': 0.14285714285714285, 'sat': 0.0, 'a': 0.0, 'walk': 0.0, 'for': 0.0, 'people': 0.0, 'out': 0.0, 'picnic': 0.0, 'around': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(tfD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'running': 0.0, 'man': 0.0, 'park': 0.0, 'children': 0.0, 'fire': 0.14285714285714285, 'in': 0.0, 'the': 0.2857142857142857, 'elder': 0.14285714285714285, 'family': 0.0, 'went': 0.0, 'were': 0.0, 'sat': 0.14285714285714285, 'a': 0.0, 'walk': 0.0, 'for': 0.0, 'people': 0.14285714285714285, 'out': 0.0, 'picnic': 0.0, 'around': 0.14285714285714285}\n"
     ]
    }
   ],
   "source": [
    "print(tfE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Data Frequency (IDF)\n",
    "- The log of the number of documents divided by the number of documents that contain the word w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([numOfWordsA, numOfWordsB, numOfWordsC, numOfWordsD, numOfWordsE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'running': 1.6094379124341003, 'man': 1.6094379124341003, 'park': 0.9162907318741551, 'children': 0.9162907318741551, 'fire': 0.9162907318741551, 'in': 0.9162907318741551, 'the': 0.0, 'elder': 1.6094379124341003, 'family': 1.6094379124341003, 'went': 0.9162907318741551, 'were': 1.6094379124341003, 'sat': 0.9162907318741551, 'a': 0.9162907318741551, 'walk': 1.6094379124341003, 'for': 0.9162907318741551, 'people': 1.6094379124341003, 'out': 1.6094379124341003, 'picnic': 1.6094379124341003, 'around': 0.9162907318741551}\n"
     ]
    }
   ],
   "source": [
    "print(idfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lastly, Find out the top TF-IDF words for the above input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "tfidfC = computeTFIDF(tfC, idfs)\n",
    "tfidfD = computeTFIDF(tfD, idfs)\n",
    "tfidfE = computeTFIDF(tfE, idfs)\n",
    "df = pd.DataFrame([tfidfA, tfidfB, tfidfC, tfidfD, tfidfE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a    around  children    elder    family      fire       for  \\\n",
      "0  0.130899  0.000000  0.000000  0.00000  0.000000  0.000000  0.130899   \n",
      "1  0.000000  0.152715  0.152715  0.00000  0.000000  0.152715  0.000000   \n",
      "2  0.101810  0.000000  0.000000  0.00000  0.178826  0.000000  0.101810   \n",
      "3  0.000000  0.000000  0.130899  0.00000  0.000000  0.000000  0.000000   \n",
      "4  0.000000  0.130899  0.000000  0.22992  0.000000  0.130899  0.000000   \n",
      "\n",
      "         in      man      out      park   people    picnic  running       sat  \\\n",
      "0  0.000000  0.22992  0.22992  0.000000  0.00000  0.000000  0.00000  0.000000   \n",
      "1  0.000000  0.00000  0.00000  0.000000  0.00000  0.000000  0.00000  0.152715   \n",
      "2  0.101810  0.00000  0.00000  0.101810  0.00000  0.178826  0.00000  0.000000   \n",
      "3  0.130899  0.00000  0.00000  0.130899  0.00000  0.000000  0.22992  0.000000   \n",
      "4  0.000000  0.00000  0.00000  0.000000  0.22992  0.000000  0.00000  0.130899   \n",
      "\n",
      "   the     walk      went     were  \n",
      "0  0.0  0.22992  0.130899  0.00000  \n",
      "1  0.0  0.00000  0.000000  0.00000  \n",
      "2  0.0  0.00000  0.101810  0.00000  \n",
      "3  0.0  0.00000  0.000000  0.22992  \n",
      "4  0.0  0.00000  0.000000  0.00000  \n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out the top TF-IDF words for the lemmatized input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_listA = nltk.word_tokenize(documentA)\n",
    "word_listB = nltk.word_tokenize(documentB)\n",
    "word_listC = nltk.word_tokenize(documentC)\n",
    "word_listD = nltk.word_tokenize(documentD)\n",
    "word_listE = nltk.word_tokenize(documentE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_documentA = ' '.join([lemmatizer.lemmatize(w) for w in word_listA])\n",
    "lemmatized_documentB = ' '.join([lemmatizer.lemmatize(w) for w in word_listB])\n",
    "lemmatized_documentC = ' '.join([lemmatizer.lemmatize(w) for w in word_listC])\n",
    "lemmatized_documentD = ' '.join([lemmatizer.lemmatize(w) for w in word_listD])\n",
    "lemmatized_documentE = ' '.join([lemmatizer.lemmatize(w) for w in word_listE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbagOfWordsA = lemmatized_documentA.split(' ')\n",
    "lbagOfWordsB = lemmatized_documentB.split(' ')\n",
    "lbagOfWordsC = lemmatized_documentC.split(' ')\n",
    "lbagOfWordsD = lemmatized_documentD.split(' ')\n",
    "lbagOfWordsE = lemmatized_documentE.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "luniqueWords = set(lbagOfWordsA).union(set(lbagOfWordsB)).union(set(lbagOfWordsC)).union(set(lbagOfWordsD)).union(set(lbagOfWordsE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnumOfWordsA = dict.fromkeys(luniqueWords, 0)\n",
    "for word in lbagOfWordsA:\n",
    "    lnumOfWordsA[word] += 1\n",
    "lnumOfWordsB = dict.fromkeys(luniqueWords, 0)\n",
    "for word in lbagOfWordsB:\n",
    "    lnumOfWordsB[word] += 1\n",
    "lnumOfWordsC = dict.fromkeys(luniqueWords, 0)\n",
    "for word in lbagOfWordsC:\n",
    "    lnumOfWordsC[word] += 1\n",
    "lnumOfWordsD = dict.fromkeys(luniqueWords, 0)\n",
    "for word in lbagOfWordsD:\n",
    "    lnumOfWordsD[word] += 1\n",
    "lnumOfWordsE = dict.fromkeys(luniqueWords, 0)\n",
    "for word in lbagOfWordsE:\n",
    "    lnumOfWordsE[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltfA = computeTF(lnumOfWordsA, lbagOfWordsA)\n",
    "ltfB = computeTF(lnumOfWordsB, lbagOfWordsB)\n",
    "ltfC = computeTF(lnumOfWordsC, lbagOfWordsC)\n",
    "ltfD = computeTF(lnumOfWordsD, lbagOfWordsD)\n",
    "ltfE = computeTF(lnumOfWordsE, lbagOfWordsE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidfs = computeIDF([lnumOfWordsA, lnumOfWordsB, lnumOfWordsC, lnumOfWordsD, lnumOfWordsE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltfidfA = computeTFIDF(ltfA, lidfs)\n",
    "ltfidfB = computeTFIDF(ltfB, lidfs)\n",
    "ltfidfC = computeTFIDF(ltfC, lidfs)\n",
    "ltfidfD = computeTFIDF(ltfD, lidfs)\n",
    "ltfidfE = computeTFIDF(ltfE, lidfs)\n",
    "ldf = pd.DataFrame([ltfidfA, ltfidfB, ltfidfC, ltfidfD, ltfidfE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a    around     child    elder    family      fire       for  \\\n",
      "0  0.130899  0.000000  0.000000  0.00000  0.000000  0.000000  0.130899   \n",
      "1  0.000000  0.152715  0.152715  0.00000  0.000000  0.152715  0.000000   \n",
      "2  0.101810  0.000000  0.000000  0.00000  0.178826  0.000000  0.101810   \n",
      "3  0.000000  0.000000  0.130899  0.00000  0.000000  0.000000  0.000000   \n",
      "4  0.000000  0.130899  0.000000  0.22992  0.000000  0.130899  0.000000   \n",
      "\n",
      "         in      man      out      park   people    picnic  running       sat  \\\n",
      "0  0.000000  0.22992  0.22992  0.000000  0.00000  0.000000  0.00000  0.000000   \n",
      "1  0.000000  0.00000  0.00000  0.000000  0.00000  0.000000  0.00000  0.152715   \n",
      "2  0.101810  0.00000  0.00000  0.101810  0.00000  0.178826  0.00000  0.000000   \n",
      "3  0.130899  0.00000  0.00000  0.130899  0.00000  0.000000  0.22992  0.000000   \n",
      "4  0.000000  0.00000  0.00000  0.000000  0.22992  0.000000  0.00000  0.130899   \n",
      "\n",
      "   the     walk      went     were  \n",
      "0  0.0  0.22992  0.130899  0.00000  \n",
      "1  0.0  0.00000  0.000000  0.00000  \n",
      "2  0.0  0.00000  0.101810  0.00000  \n",
      "3  0.0  0.00000  0.000000  0.22992  \n",
      "4  0.0  0.00000  0.000000  0.00000  \n"
     ]
    }
   ],
   "source": [
    "print(ldf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out the top TF-IDF words for the n-gram based input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "listA = [documentA]\n",
    "listB = [documentB]\n",
    "listC = [documentC]\n",
    "listD = [documentD]\n",
    "listE = [documentE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "resA = [(x, i.split()[j + 1]) for i in listA  \n",
    "       for j, x in enumerate(i.split()) if j < len(i.split()) - 1]\n",
    "resB = [(x, i.split()[j + 1]) for i in listB  \n",
    "       for j, x in enumerate(i.split()) if j < len(i.split()) - 1]\n",
    "resC = [(x, i.split()[j + 1]) for i in listC  \n",
    "       for j, x in enumerate(i.split()) if j < len(i.split()) - 1]\n",
    "resD = [(x, i.split()[j + 1]) for i in listD  \n",
    "       for j, x in enumerate(i.split()) if j < len(i.split()) - 1]\n",
    "resE = [(x, i.split()[j + 1]) for i in listE  \n",
    "       for j, x in enumerate(i.split()) if j < len(i.split()) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "runiqueWords = set(resA).union(set(resB)).union(set(resC)).union(set(resD)).union(set(resE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnumOfWordsA = dict.fromkeys(runiqueWords, 0)\n",
    "for word in resA:\n",
    "    rnumOfWordsA[word] += 1\n",
    "rnumOfWordsB = dict.fromkeys(runiqueWords, 0)\n",
    "for word in resB:\n",
    "    rnumOfWordsB[word] += 1\n",
    "rnumOfWordsC = dict.fromkeys(runiqueWords, 0)\n",
    "for word in resC:\n",
    "    rnumOfWordsC[word] += 1\n",
    "rnumOfWordsD = dict.fromkeys(runiqueWords, 0)\n",
    "for word in resD:\n",
    "    rnumOfWordsD[word] += 1\n",
    "rnumOfWordsE = dict.fromkeys(runiqueWords, 0)\n",
    "for word in resE:\n",
    "    rnumOfWordsE[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtfA = computeTF(rnumOfWordsA, resA)\n",
    "rtfB = computeTF(rnumOfWordsB, resB)\n",
    "rtfC = computeTF(rnumOfWordsC, resC)\n",
    "rtfD = computeTF(rnumOfWordsD, resD)\n",
    "rtfE = computeTF(rnumOfWordsE, resE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridfs = computeIDF([rnumOfWordsA, rnumOfWordsB, rnumOfWordsC, rnumOfWordsD, rnumOfWordsE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtfidfA = computeTFIDF(rtfA, ridfs)\n",
    "rtfidfB = computeTFIDF(rtfB, ridfs)\n",
    "rtfidfC = computeTFIDF(rtfC, ridfs)\n",
    "rtfidfD = computeTFIDF(rtfD, ridfs)\n",
    "rtfidfE = computeTFIDF(rtfE, ridfs)\n",
    "rdf = pd.DataFrame([rtfidfA, rtfidfB, rtfidfC, rtfidfD, rtfidfE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   (a, picnic)  (a, walk)  (around, the)  (children, sat)  (children, were)  \\\n",
      "0      0.00000    0.26824       0.000000         0.000000           0.00000   \n",
      "1      0.00000    0.00000       0.183258         0.321888           0.00000   \n",
      "2      0.20118    0.00000       0.000000         0.000000           0.00000   \n",
      "3      0.00000    0.00000       0.000000         0.000000           0.26824   \n",
      "4      0.00000    0.00000       0.152715         0.000000           0.00000   \n",
      "\n",
      "   (elder, people)  (family, went)  (for, a)  (in, the)  (man, went)  ...  \\\n",
      "0          0.00000         0.00000  0.152715   0.000000      0.26824  ...   \n",
      "1          0.00000         0.00000  0.000000   0.000000      0.00000  ...   \n",
      "2          0.00000         0.20118  0.114536   0.114536      0.00000  ...   \n",
      "3          0.00000         0.00000  0.000000   0.152715      0.00000  ...   \n",
      "4          0.26824         0.00000  0.000000   0.000000      0.00000  ...   \n",
      "\n",
      "   (sat, around)  (the, children)  (the, elder)  (the, family)  (the, fire)  \\\n",
      "0       0.000000         0.000000       0.00000        0.00000     0.000000   \n",
      "1       0.183258         0.183258       0.00000        0.00000     0.183258   \n",
      "2       0.000000         0.000000       0.00000        0.20118     0.000000   \n",
      "3       0.000000         0.152715       0.00000        0.00000     0.000000   \n",
      "4       0.152715         0.000000       0.26824        0.00000     0.152715   \n",
      "\n",
      "   (the, man)  (the, park)  (went, for)  (went, out)  (were, running)  \n",
      "0     0.26824     0.000000      0.00000      0.26824          0.00000  \n",
      "1     0.00000     0.000000      0.00000      0.00000          0.00000  \n",
      "2     0.00000     0.114536      0.20118      0.00000          0.00000  \n",
      "3     0.00000     0.152715      0.00000      0.00000          0.26824  \n",
      "4     0.00000     0.000000      0.00000      0.00000          0.00000  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(rdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a simple spark program to read a dataset and find the W2V Synonyms for the Top TF-IDF Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try without NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "documents = sc.textFile(\"data.txt\").map(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(numFeatures=20)\n",
    "tf = hashingTF.transform(documents)\n",
    "\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF without NLP:\n",
      "(20,[7,9,12,17],[0.1823215567939546,2.0794415416798357,0.3646431135879092,0.0])\n",
      "(20,[2,7,8,17,18],[0.6931471805599453,0.1823215567939546,0.6931471805599453,0.0,0.6931471805599453])\n",
      "(20,[0,1,7,9,12,15,17],[1.0986122886681098,0.6931471805599453,0.1823215567939546,1.3862943611198906,0.1823215567939546,1.0986122886681098,0.0])\n",
      "(20,[1,7,11,12,17],[0.6931471805599453,0.3646431135879092,1.0986122886681098,0.1823215567939546,0.0])\n",
      "(20,[2,5,8,12,17,18],[0.6931471805599453,1.0986122886681098,0.6931471805599453,0.1823215567939546,0.0,0.6931471805599453])\n"
     ]
    }
   ],
   "source": [
    "print(\"TFIDF without NLP:\")\n",
    "for each in tfidf.collect():\n",
    "    print(each)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man went out for a walk the child sat around the fire the family went for a picnic in the park the child were running in the park the elder people sat around the fire\n",
      "TFIDF with Lemmatization:\n",
      "(20,[7,9,12,17],[0.1823215567939546,2.0794415416798357,0.3646431135879092,0.0])\n",
      "(20,[2,7,8,17,18],[0.6931471805599453,0.1823215567939546,0.6931471805599453,0.0,0.6931471805599453])\n",
      "(20,[0,1,7,9,12,15,17],[1.0986122886681098,0.6931471805599453,0.1823215567939546,1.3862943611198906,0.1823215567939546,1.0986122886681098,0.0])\n",
      "(20,[1,7,11,12,17],[0.6931471805599453,0.3646431135879092,1.0986122886681098,0.1823215567939546,0.0])\n",
      "(20,[2,5,8,12,17,18],[0.6931471805599453,1.0986122886681098,0.6931471805599453,0.1823215567939546,0.0,0.6931471805599453])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "documents = sc.textFile(\"data.txt\").map(lambda line: line.split(\" \"))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_list = list(map(' '.join, documents.collect()))\n",
    "word_list1 = ''\n",
    "for i in word_list:\n",
    "    word_list1 = word_list1 + ' ' + i\n",
    "word_list2 = nltk.word_tokenize(word_list1)\n",
    "lemmatized_document = ' '.join([lemmatizer.lemmatize(w) for w in word_list2])\n",
    "print(lemmatized_document)\n",
    "\n",
    "f = open(\"data1.txt\", \"w+\")\n",
    "f.write('' + lemmatized_document)\n",
    "f.close()\n",
    "\n",
    "document1 = sc.textFile(\"data.txt\").map(lambda line: line.split(\" \"))\n",
    "\n",
    "hashingTF = HashingTF(numFeatures=20)\n",
    "tf = hashingTF.transform(document1)\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "print(\"TFIDF with Lemmatization:\")\n",
    "for each in tfidf.collect():\n",
    "    print(each)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try with N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Features : \n",
      " ['around fire', 'child running', 'child sat', 'elder people', 'family went', 'fire family', 'man went', 'park child', 'park elder', 'people sat', 'picnic park', 'running park', 'sat around', 'walk child', 'went picnic', 'went walk']\n",
      "\n",
      "\n",
      "X1 : \n",
      " [[2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1]]\n",
      "\n",
      "\n",
      "Scores : \n",
      " [[0.22941573 0.22941573 0.22941573 0.22941573 0.22941573 0.22941573\n",
      "  0.22941573 0.22941573 0.22941573 0.22941573 0.22941573 0.22941573\n",
      "  0.45883147 0.22941573 0.22941573 0.22941573]]\n",
      "\n",
      "\n",
      "Words head : \n",
      "              term      rank\n",
      "12     sat around  0.458831\n",
      "0     around fire  0.229416\n",
      "1   child running  0.229416\n",
      "2       child sat  0.229416\n",
      "3    elder people  0.229416\n",
      "4     family went  0.229416\n",
      "5     fire family  0.229416\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "txt1 = []\n",
    "with open('data1.txt') as file:\n",
    "    txt1 = file.readlines()\n",
    "\n",
    "def remove_string_special_characters(s):\n",
    "    stripped = re.sub('[^a-zA-z\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '':\n",
    "        return stripped.lower()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "your_list = ['skills', 'ability', 'job', 'description']\n",
    "for i, line in enumerate(txt1):\n",
    "    txt1[i] = ' '.join([x for\n",
    "                        x in nltk.word_tokenize(line) if\n",
    "                        (x not in stop_words) and (x not in your_list)])\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X1 = vectorizer.fit_transform(txt1)\n",
    "features = (vectorizer.get_feature_names())\n",
    "print(\"\\n\\nFeatures : \\n\", features)\n",
    "print(\"\\n\\nX1 : \\n\", X1.toarray())\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3, 3))\n",
    "X2 = vectorizer.fit_transform(txt1)\n",
    "scores = (X2.toarray())\n",
    "print(\"\\n\\nScores : \\n\", scores)\n",
    "\n",
    "# Getting top ranking features \n",
    "sums = X2.sum(axis=0)\n",
    "data1 = []\n",
    "for col, term in enumerate(features):\n",
    "    data1.append((term, sums[0, col]))\n",
    "ranking = pd.DataFrame(data1, columns=['term', 'rank'])\n",
    "words = (ranking.sort_values('rank', ascending=False))\n",
    "print(\"\\n\\nWords head : \\n\", words.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
